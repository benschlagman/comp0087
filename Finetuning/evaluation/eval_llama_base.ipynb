{"cells":[{"cell_type":"markdown","source":["This notebook was run in Google Colab. To run, connect to a GPU, such as the V100, and upload the necessary datasets. Access to the llama-2-7b model is controlled by Meta. To request access, follow this link: https://huggingface.co/meta-llama/Llama-2-7b. Once permission is granted, you will need to log in to Hugging Face with a valid token. The code for evaluation was partially taken from others papers and merged together. Boilerplate code was typically generated using Claude 3 Opus and GPT-4 and then adjusted for our specific use case. Sources of pre-built functions are referenced within the cell.\n"],"metadata":{"id":"jtF6ZoVLuEhs"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"MxNjwZuelMk4"},"outputs":[],"source":["#%%capture\n","!pip install accelerate peft bitsandbytes transformers trl sacrebleu rouge\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fa9lFkw-lCln"},"outputs":[],"source":["import torch\n","from transformers import AutoModelForCausalLM, AutoTokenizer\n","from typing import List, Tuple\n","\n","from huggingface_hub import login\n","login()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gDbqLNlWhDwl"},"outputs":[],"source":["# Check if a GPU is available\n","if not torch.cuda.is_available():\n","    raise EnvironmentError(\"This script requires a GPU to run.\")\n","\n","# Constants\n","MAX_INPUT_TOKEN_LENGTH = 4096\n","DEFAULT_MAX_NEW_TOKENS = 50\n","\n","# Load the model and tokenizer\n","model_id = \"meta-llama/Llama-2-7b-chat-hf\"\n","model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.float16, device_map=\"auto\")\n","tokenizer = AutoTokenizer.from_pretrained(model_id)\n","tokenizer.use_default_system_prompt = False\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hQnQGcGM4uFM"},"outputs":[],"source":["# IMPORTING LLAMA OUTPUTS FILE\n","import csv\n","\n","model_response_flattened = []\n","conversation_golden_responses_flattened = []\n","\n","filename = \"llama_outputs.csv\"\n","\n","with open(filename, 'r', newline='', encoding='utf-8') as csvfile:\n","\n","    csvreader = csv.reader(csvfile)\n","    next(csvreader)\n","\n","    for row in csvreader:\n","      model_response_flattened.append(row[0])\n","      conversation_golden_responses_flattened.append(row[1])\n","\n","print(\"CSV file data imported successfully.\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1xRqjcEkB65i"},"outputs":[],"source":["print(conversation_golden_responses_flattened)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"g__iZmV5O4-m"},"outputs":[],"source":["from nltk.translate.bleu_score import sentence_bleu, corpus_bleu\n","from nltk.translate.bleu_score import SmoothingFunction\n","\n","def calculate_bleu_scores(model_responses, golden_responses, is_corpus=False):\n","    assert len(model_responses) == len(golden_responses), \"The lengths of model responses and golden responses should match.\"\n","\n","    bleu_1_scores = []\n","    bleu_2_scores = []\n","    bleu_3_scores = []\n","    bleu_4_scores = []\n","\n","    # Calculate sentence BLEU scores for each response\n","    for model_response, golden_response in zip(model_responses, golden_responses):\n","        reference = [golden_response.split()]  # Tokenize the golden response\n","        candidate = model_response.split()     # Tokenize the model response\n","\n","        # Calculate and store BLEU scores for each sentence\n","        bleu_1_scores.append(sentence_bleu(reference, candidate, weights=(1, 0, 0, 0), smoothing_function=SmoothingFunction().method1))\n","        bleu_2_scores.append(sentence_bleu(reference, candidate, weights=(0.5, 0.5, 0, 0), smoothing_function=SmoothingFunction().method1))\n","        bleu_3_scores.append(sentence_bleu(reference, candidate, weights=(0.33, 0.33, 0.33, 0), smoothing_function=SmoothingFunction().method1))\n","        bleu_4_scores.append(sentence_bleu(reference, candidate, weights=(0.25, 0.25, 0.25, 0.25), smoothing_function=SmoothingFunction().method1))\n","\n","    # Calculate average BLEU scores across all responses\n","    avg_bleu_1 = sum(bleu_1_scores) / len(bleu_1_scores)\n","    avg_bleu_2 = sum(bleu_2_scores) / len(bleu_2_scores)\n","    avg_bleu_3 = sum(bleu_3_scores) / len(bleu_3_scores)\n","    avg_bleu_4 = sum(bleu_4_scores) / len(bleu_4_scores)\n","\n","    return avg_bleu_1, avg_bleu_2, avg_bleu_3, avg_bleu_4"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xutCvKa7O98P"},"outputs":[],"source":["avg_bleu_1, avg_bleu_2, avg_bleu_3, avg_bleu_4 = calculate_bleu_scores(model_response_flattened, conversation_golden_responses_flattened)\n","print(\"Average BLEU-1 score:\", avg_bleu_1 * 100)\n","print(\"Average BLEU-2 score:\", avg_bleu_2 * 100)\n","print(\"Average BLEU-3 score:\", avg_bleu_3 * 100)\n","print(\"Average BLEU-4 score:\", avg_bleu_4 * 100)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"n7NZAPphFphf"},"outputs":[],"source":["#%%capture\n","!pip install accelerate rouge_score\n","from rouge_score import rouge_scorer\n","\n","# Initialize the scorer\n","scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n","\n","# Initialize sums for each ROUGE score\n","sum_rougeL_precision, sum_rougeL_recall, sum_rougeL_fmeasure = 0, 0, 0\n","\n","# Calculate scores for each sentence pair\n","for m, c in zip(model_response_flattened, conversation_golden_responses_flattened):\n","    score = scorer.score(m, c)\n","\n","    # Accumulate the scores\n","    sum_rougeL_precision += score[\"rougeL\"].precision\n","    sum_rougeL_recall += score[\"rougeL\"].recall\n","    sum_rougeL_fmeasure += score[\"rougeL\"].fmeasure\n","\n","# Calculate the averages\n","avg_rougeL_precision = sum_rougeL_precision / len(model_response_flattened)\n","avg_rougeL_recall = sum_rougeL_recall / len(model_response_flattened)\n","avg_rougeL_fmeasure = sum_rougeL_fmeasure / len(model_response_flattened)\n","\n","# Print the average scores\n","print(f'Average ROUGE-L Precision: {avg_rougeL_precision}')\n","print(f'Average ROUGE-L Recall: {avg_rougeL_recall}')\n","print(f'Average ROUGE-L F-measure: {avg_rougeL_fmeasure}')\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7AAPw2lMBzA7"},"outputs":[],"source":["# METEOR:\n","#%%capture\n","!pip install nltk\n","\n","import nltk\n","nltk.download(\"wordnet\")\n","nltk.download(\"punkt\")\n","\n","from nltk.translate import meteor\n","from nltk.tokenize import word_tokenize\n","\n","\n","def calculate_meteor(candidate, reference):\n","  '''\n","  candidate, reference: tokenized list of words in the sentence\n","  '''\n","  mt_list = []\n","  mt_sum = 0\n","  for c, r in zip(candidate, reference):\n","    r_tokenized = word_tokenize(r)\n","    c_tokenized = word_tokenize(c)\n","    meteor_score = round(meteor([c_tokenized], r_tokenized), 4)\n","    #print(\"Model output: \", c)\n","    #print(\"Gold reference: \", r)\n","    #print(\"Meteor score: \", meteor_score)\n","    mt_list.append(meteor_score)\n","    mt_sum += meteor_score\n","    #print(\"\\n\")\n","  avg_mt = mt_sum / len(mt_list)\n","  print(f\"Total average meteor score: {str(avg_mt)}\")\n","\n","  return meteor_score\n","\n","calculate_meteor(model_response_flattened, conversation_golden_responses_flattened)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wTmBcDBiB5iG"},"outputs":[],"source":["# PERPLEXITY: https://huggingface.co/docs/transformers/perplexity\n","\n","def calculate_perplexity(response: str, model, tokenizer, max_length):\n","    encodings = tokenizer(response, return_tensors=\"pt\")\n","    seq_len = encodings.input_ids.size(1)\n","    stride = 512\n","    nlls = []\n","\n","    for begin_loc in range(0, seq_len, stride):\n","        end_loc = min(begin_loc + max_length, seq_len)\n","        input_ids = encodings.input_ids[:, begin_loc:end_loc].to(model.device)\n","        target_ids = input_ids.clone()\n","        target_ids[:, :-stride] = -100  # ignore the shifted tokens for loss calculation\n","\n","        with torch.no_grad():\n","            outputs = model(input_ids, labels=target_ids)\n","            neg_log_likelihood = outputs.loss\n","            nlls.append(neg_log_likelihood)\n","\n","    ppl = torch.exp(torch.stack(nlls).mean())\n","    return ppl.item()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bH57dJyFJSGH"},"outputs":[],"source":["perplexities = []\n","\n","for response in model_response_flattened:\n","    ppl = calculate_perplexity(response, model, tokenizer, max_length=512)\n","    perplexities.append(ppl)\n","\n","print(perplexities)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZfXAhldhMlhh"},"outputs":[],"source":["avg_perplexity = sum(perplexities) / len(perplexities)\n","print(avg_perplexity)"]},{"cell_type":"code","source":["# Clone the Distinct-N repository\n","!git clone https://github.com/neural-dialogue-metrics/Distinct-N.git\n","%cd Distinct-N\n","\n","from distinct_n.utils import ngrams\n","\n","def distinct_n_sentence_level(sentence, n):\n","    \"\"\"\n","    Compute distinct-N for a single sentence.\n","    :param sentence: a list of words.\n","    :param n: int, ngram.\n","    :return: float, the metric value.\n","    \"\"\"\n","    if len(sentence) == 0:\n","        return 0.0  # Prevent a zero division\n","    distinct_ngrams = set(ngrams(sentence, n))\n","    return len(distinct_ngrams) / len(sentence)\n","\n","def distinct_n_corpus_level(sentences, n):\n","    \"\"\"\n","    Compute average distinct-N of a list of sentences (the corpus).\n","    :param sentences: a list of sentence.\n","    :param n: int, ngram.\n","    :return: float, the average value.\n","    \"\"\"\n","    return sum(distinct_n_sentence_level(sentence, n) for sentence in sentences) / len(sentences)"],"metadata":{"id":"hHz516QPeaCG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["distinct_1_list = []\n","\n","for response in model_response_flattened:\n","    d_1 = distinct_n_sentence_level(response, 1)  # or adjust max_length as needed\n","    distinct_1_list.append(d_1)\n","\n","# Now, 'perplexities' contains the perplexity for each response.\n","# You can print them out or analyze them further as needed.\n","print(distinct_1_list)"],"metadata":{"id":"q_IMsu08h6R8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["distinct_2_list = []\n","\n","for response in model_response_flattened:\n","    d_2 = distinct_n_sentence_level(response, 2)  # or adjust max_length as needed\n","    distinct_2_list.append(d_2)\n","\n","# Now, 'perplexities' contains the perplexity for each response.\n","# You can print them out or analyze them further as needed.\n","print(distinct_2_list)"],"metadata":{"id":"EgQlmHj5iQEF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["d_1 = sum(distinct_1_list) / len(distinct_1_list)\n","print(d_1)"],"metadata":{"id":"LDVbzo_jlGPt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["d_2 = sum(distinct_2_list) / len(distinct_2_list)\n","print(d_2)"],"metadata":{"id":"l0CJ_mFflTa_"},"execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"V100","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}