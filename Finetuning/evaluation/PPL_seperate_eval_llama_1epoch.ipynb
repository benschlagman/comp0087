{"cells":[{"cell_type":"markdown","source":["This notebook was run in Google Colab. To run, connect to a GPU, such as the V100, and upload the necessary datasets. Access to the llama-2-7b model is controlled by Meta. To request access, follow this link: https://huggingface.co/meta-llama/Llama-2-7b. Once permission is granted, you will need to log in to Hugging Face with a valid token. The code for evaluation was partially taken from others papers and merged together. Boilerplate code was typically generated using Claude 3 Opus and GPT-4 and then adjusted for our specific use case. Sources of pre-built functions are referenced within the cell.\n","\n"],"metadata":{"id":"vY_JBN5stMpt"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"MxNjwZuelMk4"},"outputs":[],"source":["#%%capture\n","!pip install accelerate peft bitsandbytes transformers trl sacrebleu rouge\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fa9lFkw-lCln"},"outputs":[],"source":["import torch\n","from transformers import AutoModelForCausalLM, AutoTokenizer\n","from typing import List, Tuple\n","\n","from huggingface_hub import login\n","login()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gDbqLNlWhDwl"},"outputs":[],"source":["# Check if a GPU is available\n","if not torch.cuda.is_available():\n","    raise EnvironmentError(\"This script requires a GPU to run.\")\n","\n","# Constants\n","MAX_INPUT_TOKEN_LENGTH = 4096\n","DEFAULT_MAX_NEW_TOKENS = 50\n","\n","# Load the model and tokenizer\n","model_id = \"benschlagman/llama-2-7b-chat-esconv\"\n","model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.float16, device_map=\"auto\")\n","tokenizer = AutoTokenizer.from_pretrained(model_id)\n","tokenizer.use_default_system_prompt = False\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hQnQGcGM4uFM"},"outputs":[],"source":["# IMPORTING LLAMA OUTPUTS FILE\n","\n","import csv\n","\n","model_response_flattened = []\n","conversation_golden_responses_flattened = []\n","\n","\n","filename = \"raw_finetuned_output_1e.csv\"\n","\n","\n","with open(filename, 'r', newline='', encoding='utf-8') as csvfile:\n","    csvreader = csv.reader(csvfile)\n","    next(csvreader)\n","\n","    for row in csvreader:\n","      model_response_flattened.append(row[0])\n","      conversation_golden_responses_flattened.append(row[1])\n","\n","print(\"CSV file data imported successfully.\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1xRqjcEkB65i"},"outputs":[],"source":["print(conversation_golden_responses_flattened)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wTmBcDBiB5iG"},"outputs":[],"source":["# PERPLEXITY: https://huggingface.co/docs/transformers/perplexity\n","\n","def calculate_perplexity(response: str, model, tokenizer, max_length):\n","    encodings = tokenizer(response, return_tensors=\"pt\")\n","    seq_len = encodings.input_ids.size(1)\n","    stride = 512\n","    nlls = []\n","\n","    for begin_loc in range(0, seq_len, stride):\n","        end_loc = min(begin_loc + max_length, seq_len)\n","        input_ids = encodings.input_ids[:, begin_loc:end_loc].to(model.device)\n","        target_ids = input_ids.clone()\n","        target_ids[:, :-stride] = -100  # ignore the shifted tokens for loss calculation\n","\n","        with torch.no_grad():\n","            outputs = model(input_ids, labels=target_ids)\n","            neg_log_likelihood = outputs.loss\n","            nlls.append(neg_log_likelihood)\n","\n","    ppl = torch.exp(torch.stack(nlls).mean())\n","    return ppl.item()"]},{"cell_type":"code","source":["perplexities = []\n","\n","for response in model_response_flattened:\n","    ppl = calculate_perplexity(response, model, tokenizer, max_length=512)\n","    perplexities.append(ppl)\n","\n","print(perplexities)"],"metadata":{"id":"sw2Wu8uxNBM8"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZfXAhldhMlhh"},"outputs":[],"source":["perplexities"]},{"cell_type":"code","source":["import math\n","\n","#Filter out NaN values\n","filtered_perplexities = [value for value in perplexities if not math.isnan(value)]\n","\n","#Compute the average of the filtered list\n","average = sum(filtered_perplexities) / len(filtered_perplexities) if filtered_perplexities else float('nan')\n","print(average)"],"metadata":{"id":"p4M4Oeg2OFSD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["avg_perplexity = sum(perplexities) / len(perplexities)\n","print(avg_perplexity)"],"metadata":{"id":"HFTKMj0eN3GH"},"execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","machine_shape":"hm","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}