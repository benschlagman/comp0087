{"cells":[{"cell_type":"markdown","source":["This notebook was run in Google Colab. To run every cell you need to upload the data set and prompts into the same folder. The code for evaluation was partially taken from others papers and merged together. Boilerplate code was typically generated using Claude 3 Opus and GPT-4 and then adjusted for our specific use case."],"metadata":{"id":"CN_JbCSlZO-g"}},{"cell_type":"markdown","metadata":{"id":"R4ycZiQrr0N3"},"source":["# Install dependencies"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gs55JoO4rsmF","outputId":"603b9553-ae83-46ed-943f-4299ede5b461","executionInfo":{"status":"ok","timestamp":1712178220871,"user_tz":-60,"elapsed":42956,"user":{"displayName":"Niclas Grießhaber","userId":"09526291260391729561"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting sacrebleu\n","  Downloading sacrebleu-2.4.1-py3-none-any.whl (106 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.6/106.6 kB\u001b[0m \u001b[31m1.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting portalocker (from sacrebleu)\n","  Downloading portalocker-2.8.2-py3-none-any.whl (17 kB)\n","Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (2023.12.25)\n","Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (0.9.0)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (1.25.2)\n","Collecting colorama (from sacrebleu)\n","  Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n","Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (4.9.4)\n","Installing collected packages: portalocker, colorama, sacrebleu\n","Successfully installed colorama-0.4.6 portalocker-2.8.2 sacrebleu-2.4.1\n","Collecting rouge_score\n","  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from rouge_score) (1.4.0)\n","Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from rouge_score) (3.8.1)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from rouge_score) (1.25.2)\n","Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from rouge_score) (1.16.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->rouge_score) (8.1.7)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->rouge_score) (1.3.2)\n","Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk->rouge_score) (2023.12.25)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk->rouge_score) (4.66.2)\n","Building wheels for collected packages: rouge_score\n","  Building wheel for rouge_score (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for rouge_score: filename=rouge_score-0.1.2-py3-none-any.whl size=24933 sha256=d11ac83beef9ac343585d0189c91bfb15df365838d7acdfad0297c297a9eb553\n","  Stored in directory: /root/.cache/pip/wheels/5f/dd/89/461065a73be61a532ff8599a28e9beef17985c9e9c31e541b4\n","Successfully built rouge_score\n","Installing collected packages: rouge_score\n","Successfully installed rouge_score-0.1.2\n"]}],"source":["!pip install sacrebleu\n","!pip install rouge_score\n","\n","# METEOR:\n","#%%capture\n","#!pip install nltk"]},{"cell_type":"markdown","metadata":{"id":"Js_Q8Rf77Hzr"},"source":["# Import libraries"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pL8RTIAd6_zV","outputId":"dcdc8661-a68e-46e0-f5c6-9ce62ce32a3a","executionInfo":{"status":"ok","timestamp":1712178232115,"user_tz":-60,"elapsed":11248,"user":{"displayName":"Niclas Grießhaber","userId":"09526291260391729561"}}},"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n"]}],"source":["# Import libraries\n","import pathlib\n","import textwrap\n","import json\n","import pickle\n","import sacrebleu\n","import numpy as np\n","import google.generativeai as genai\n","from IPython.display import display\n","from IPython.display import Markdown\n","import nltk\n","from nltk.translate import meteor\n","from nltk.tokenize import word_tokenize\n","from nltk.translate.bleu_score import sentence_bleu, corpus_bleu\n","from nltk.translate.bleu_score import SmoothingFunction\n","nltk.download(\"wordnet\")\n","nltk.download(\"punkt\")\n","from rouge_score import rouge_scorer\n","\n","\n","# Import library to call Google API key\n","from google.colab import userdata"]},{"cell_type":"markdown","metadata":{"id":"tJ1uiJMkElmj"},"source":["# Set up API key"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"z-XjTwUAEnNL"},"outputs":[],"source":["# Or use `os.getenv('GOOGLE_API_KEY')` to fetch an environment variable.\n","GOOGLE_API_KEY=userdata.get('GOOGLE_API_KEY')\n","genai.configure(api_key=GOOGLE_API_KEY)"]},{"cell_type":"markdown","metadata":{"id":"kiVZLHfA7PvO"},"source":["# Define functions"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PnW5o4FT7EUg"},"outputs":[],"source":["# Function that retrieves the whole dialog chat from a single conversation\n","def sort_and_track_dialog(dialog):\n","    seeker_list = []\n","    supporter_list = []\n","    first_speaker = None  # Track who starts the conversation\n","\n","    current_speaker = None\n","    current_content = \"\"\n","\n","    for i, entry in enumerate(dialog):\n","        speaker = entry['speaker']\n","        content = entry['content']\n","\n","        # Determine the first speaker\n","        if i == 0:\n","            first_speaker = speaker\n","\n","        if speaker == current_speaker:\n","            # Concatenate content if the same speaker continues\n","            current_content += \" \" + content\n","        else:\n","            # Save the previous speaker's content if any\n","            if current_content:\n","                if current_speaker == 'seeker':\n","                    seeker_list.append(current_content)\n","                elif current_speaker == 'supporter':\n","                    supporter_list.append(current_content)\n","\n","            # Update the current speaker and content with the new entry\n","            current_speaker = speaker\n","            current_content = content\n","\n","    # Add the last speaker's content to the respective list\n","    if current_content:\n","        if current_speaker == 'seeker':\n","            seeker_list.append(current_content)\n","        elif current_speaker == 'supporter':\n","            supporter_list.append(current_content)\n","\n","    return seeker_list, supporter_list, first_speaker"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jcPtcWcM8FXv"},"outputs":[],"source":["def create_chat_history(prompt, model_start, seeker_list, supporter_list, first_speaker):\n","\n","    # Initialize chat_history with the prompt and initial model message\n","    chat_history = [\n","        {\"parts\": [{\"text\": prompt}], \"role\": \"user\"},\n","        {\"parts\": [{\"text\": model_start}], \"role\": \"model\"}\n","    ]\n","\n","    # Adjust initial model message if the first speaker is 'supporter'\n","    if first_speaker == \"supporter\" and supporter_list:\n","        # Add the first message of the supporter_list to the model_start text\n","        chat_history[1][\"parts\"][0][\"text\"] += \"\\n\" + supporter_list[0]\n","        supporter_start_idx = 1  # Start from the second message for supporter\n","    else:\n","        supporter_start_idx = 0  # Start from the first message for supporter\n","\n","    # Determine the starting point for seeker messages\n","    seeker_start_idx = 0\n","\n","    # Calculate the total iterations needed\n","    max_iterations = max(len(seeker_list) - seeker_start_idx, len(supporter_list) - supporter_start_idx)\n","\n","    # Alternating between seeker and supporter messages\n","    for i in range(max_iterations):\n","        if i + seeker_start_idx < len(seeker_list):\n","            chat_history.append({\n","                \"parts\": [{\"text\": seeker_list[i + seeker_start_idx]}],\n","                \"role\": \"user\"\n","            })\n","        if i + supporter_start_idx < len(supporter_list):\n","            chat_history.append({\n","                \"parts\": [{\"text\": supporter_list[i + supporter_start_idx]}],\n","                \"role\": \"model\"\n","            })\n","\n","    return chat_history"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"92jQsgHN0jZb"},"outputs":[],"source":["def generate_model_responses(dialog, prompt, model_start):\n","  # Get seeker_list, supporter_list and who starts the conversation\n","  seeker_list, supporter_list, first_speaker = sort_and_track_dialog(dialog)\n","\n","  # Create chat history\n","  chat_history = create_chat_history(prompt, model_start, seeker_list, supporter_list, first_speaker)\n","\n","  # Initialise model responses\n","  model_responses = []\n","  golden_responses = []\n","\n","  # Determine max index\n","  max_index = len(chat_history) - 4\n","\n","  # Loop to generate model responses\n","  for i in range(0, max_index, 2):\n","    try:\n","      # Current history\n","      current_history = chat_history[:i+2]\n","\n","      # Initiate model\n","      model = genai.GenerativeModel(model_name=\"gemini-pro\", generation_config=generation_config, safety_settings=safety_settings)\n","\n","      # Clear chat history\n","      chat = model.start_chat(history=current_history)\n","\n","      # Send message\n","      current_message = chat_history[i+2]['parts'][0]['text']\n","\n","      # Send message and get model response\n","      model_response = chat.send_message(current_message).text\n","\n","      # Save model responses in list if no exception occurs\n","      model_responses.append(model_response)\n","\n","      # Save golden responses in list\n","      golden_response = chat_history[i+3]['parts'][0]['text']\n","      golden_responses.append(golden_response)\n","\n","      # Print chat history\n","      #print(f'Golden response: \\n {golden_response}')\n","      #print(f'Model response: \\n {model_response}')\n","\n","    except Exception as e:\n","      # Log the error, e.g., print or save to a log file\n","      print(f\"An error occurred: {e}\")\n","      # Optionally, break out of the loop or continue depending on the desired behavior\n","      break  # or use `continue` to skip adding responses for the current iteration and move to the next\n","\n","  return model_responses, golden_responses"]},{"cell_type":"markdown","metadata":{"id":"0nZYwoHj0y23"},"source":["# Gemini model configuration"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2v0iwWBi01Vs"},"outputs":[],"source":["# Model configuration\n","generation_config = {\n","    \"temperature\":0.5, # temperature parameter\n","    \"top_p\":1,\n","    \"top_k\":1,\n","    \"max_output_tokens\":1000,\n","}\n","\n","# Safety Settings\n","safety_settings = [\n","    {\n","        \"category\": \"HARM_CATEGORY_HARASSMENT\",\n","        \"threshold\": \"BLOCK_NONE\"\n","    },\n","    {\n","        \"category\": \"HARM_CATEGORY_HATE_SPEECH\",\n","        \"threshold\": \"BLOCK_NONE\"\n","    },\n","    {\n","        \"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\",\n","        \"threshold\": \"BLOCK_NONE\"\n","    },\n","    {\n","        \"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\",\n","        \"threshold\": \"BLOCK_NONE\"\n","    },\n","]"]},{"cell_type":"markdown","metadata":{"id":"5x9Pwx1u7SzS"},"source":["# Hyperparameters"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4Dbln_2a7Ru5","colab":{"base_uri":"https://localhost:8080/","height":216},"executionInfo":{"status":"error","timestamp":1712177623802,"user_tz":-60,"elapsed":5,"user":{"displayName":"Niclas Grießhaber","userId":"09526291260391729561"}},"outputId":"8a3a60ee-b84f-4420-c360-c433b4778d6c"},"outputs":[{"output_type":"error","ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: '40_example_conversations_prompt.txt'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-10-7fa60523bc78>\u001b[0m in \u001b[0;36m<cell line: 11>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mfile_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'40_example_conversations_prompt'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mfile_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf'{file_name}.txt'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0mprompt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '40_example_conversations_prompt.txt'"]}],"source":["# File path\n","# You need to upload some of the following files to run it.\n","file_path = 'ESConv.json'\n","\n","# Load the JSON data\n","with open(file_path, 'r') as file:\n","    data = json.load(file)\n","\n","# Get prompt\n","file_name = '40_example_conversations_prompt'\n","file_path = f'{file_name}.txt'\n","with open(file_path, 'r') as file:\n","    prompt = file.read()\n","\n","# First model response for chat history\n","model_start = \"I understand that I should provide psychological help and that the previous message provides a suitable guideline. In the following conversation, I will only reply in 1-2 sentences:\"\n","\n","# Save path for model and golden responses\n","save_golden_responses = f'{file_name}_all_golden_responses.pkl'\n","save_model_responses = f'{file_name}_all_model_responses.pkl'"]},{"cell_type":"markdown","metadata":{"id":"FMRPaC9cUzvs"},"source":["# Generate model and golden responses for a single conversation"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"leCzW_su7Z-N"},"outputs":[],"source":["# Get dialog of conversation\n","dialog = data[0]['dialog']\n","\n","# Get seeker_list, supporter_list and who starts the conversation\n","seeker_list, supporter_list, first_speaker = sort_and_track_dialog(dialog)\n","\n","# Create chat history\n","chat_history = create_chat_history(prompt, model_start, seeker_list, supporter_list, first_speaker)\n","\n","# Generate model and golden responses\n","model_responses, golden_responses = generate_model_responses(dialog, prompt, model_start)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2,"status":"ok","timestamp":1711982281596,"user":{"displayName":"Niclas Grießhaber","userId":"09526291260391729561"},"user_tz":-60},"id":"GcsxEziHHen4","outputId":"f8cc7e2b-8da3-4578-b2ba-7c2d05499eb2"},"outputs":[{"data":{"text/plain":["['Hello there. I am here to provide emotional support. How can I assist you today?',\n"," \"It's understandable to feel anxious about quitting a well-paying job. Let's explore your concerns and identify strategies to manage your anxiety.\",\n"," \"It's understandable that dealing with people in hard financial situations can be upsetting. Have you considered any other career options that might be less stressful but still financially rewarding?\",\n"," \"It must be heartbreaking to know that you can't help everyone as much as you'd like to.\",\n"," \"It's important to prioritize your own well-being. If your job is causing you significant stress and anxiety, it may be time to consider other options.\",\n"," \"That is something to consider.  It's important to weigh the pros and cons of staying in a stressful job versus finding a less stressful job with lower pay.\",\n"," 'Can you think of any positive aspects of your job that outweigh the negative aspects?',\n"," \"It's okay to wonder if it's for you. Most people do at one point or another. What are the pros and cons of staying in your current role?\"]"]},"execution_count":14,"metadata":{},"output_type":"execute_result"}],"source":["model_responses"]},{"cell_type":"markdown","metadata":{"id":"NLLrmeGlCdBB"},"source":["# Load the test data set to generate model responses"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cPzLcvNiCgDR"},"outputs":[],"source":["# Replace 'file_path' with the path to your text file\n","file_path = 'testdata.txt'\n","\n","# Initialize a list to store the JSON objects\n","testdata = []\n","\n","# Open the file and read line by line\n","with open(file_path, 'r') as file:\n","    for line in file:\n","        # Parse each line as a JSON object and append to the list\n","        json_line = json.loads(line.strip())\n","        testdata.append(json_line)\n","\n","# Renaming the keys as per the requirements\n","for conversation in testdata:\n","    for message in conversation['dialog']:\n","        # Renaming 'text' to 'content'\n","        message['content'] = message.pop('text')\n","        # Renaming 'speaker' values\n","        if message['speaker'] == 'sys':\n","            message['speaker'] = 'supporter'\n","        elif message['speaker'] == 'usr':\n","            message['speaker'] = 'seeker'"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3,"status":"ok","timestamp":1712158464708,"user":{"displayName":"Niclas Grießhaber","userId":"09526291260391729561"},"user_tz":-60},"id":"kd30qjcUE6Z5","outputId":"2d723797-a949-49fd-86f6-f198ff0e296d"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["195"]},"metadata":{},"execution_count":11}],"source":["len(testdata)"]},{"cell_type":"markdown","metadata":{"id":"e7BgJl6BSSJb"},"source":["# Generate model responses and golden responses for all"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":5140797,"status":"ok","timestamp":1712163607414,"user":{"displayName":"Niclas Grießhaber","userId":"09526291260391729561"},"user_tz":-60},"id":"UmV66_flSPns","outputId":"01646914-d71b-48c1-a91f-7276f740353c"},"outputs":[{"output_type":"stream","name":"stdout","text":["Starting to generate model responses for test conversation 0 of 195\n","Starting to generate model responses for test conversation 1 of 195\n","Starting to generate model responses for test conversation 2 of 195\n","Starting to generate model responses for test conversation 3 of 195\n","Starting to generate model responses for test conversation 4 of 195\n","Starting to generate model responses for test conversation 5 of 195\n","Starting to generate model responses for test conversation 6 of 195\n","Starting to generate model responses for test conversation 7 of 195\n","Starting to generate model responses for test conversation 8 of 195\n","Starting to generate model responses for test conversation 9 of 195\n","Starting to generate model responses for test conversation 10 of 195\n","Starting to generate model responses for test conversation 11 of 195\n","Starting to generate model responses for test conversation 12 of 195\n","Starting to generate model responses for test conversation 13 of 195\n","Starting to generate model responses for test conversation 14 of 195\n","Starting to generate model responses for test conversation 15 of 195\n","Starting to generate model responses for test conversation 16 of 195\n","Starting to generate model responses for test conversation 17 of 195\n","Starting to generate model responses for test conversation 18 of 195\n","Starting to generate model responses for test conversation 19 of 195\n","Starting to generate model responses for test conversation 20 of 195\n","Starting to generate model responses for test conversation 21 of 195\n","Starting to generate model responses for test conversation 22 of 195\n","Starting to generate model responses for test conversation 23 of 195\n","Starting to generate model responses for test conversation 24 of 195\n","Starting to generate model responses for test conversation 25 of 195\n","Starting to generate model responses for test conversation 26 of 195\n","Starting to generate model responses for test conversation 27 of 195\n","Starting to generate model responses for test conversation 28 of 195\n","Starting to generate model responses for test conversation 29 of 195\n","Starting to generate model responses for test conversation 30 of 195\n","Starting to generate model responses for test conversation 31 of 195\n","Starting to generate model responses for test conversation 32 of 195\n","Starting to generate model responses for test conversation 33 of 195\n","Starting to generate model responses for test conversation 34 of 195\n","Starting to generate model responses for test conversation 35 of 195\n","Starting to generate model responses for test conversation 36 of 195\n","Starting to generate model responses for test conversation 37 of 195\n","Starting to generate model responses for test conversation 38 of 195\n","Starting to generate model responses for test conversation 39 of 195\n","Starting to generate model responses for test conversation 40 of 195\n","Starting to generate model responses for test conversation 41 of 195\n","Starting to generate model responses for test conversation 42 of 195\n","Starting to generate model responses for test conversation 43 of 195\n","Starting to generate model responses for test conversation 44 of 195\n","Starting to generate model responses for test conversation 45 of 195\n","Starting to generate model responses for test conversation 46 of 195\n","Starting to generate model responses for test conversation 47 of 195\n","Starting to generate model responses for test conversation 48 of 195\n","Starting to generate model responses for test conversation 49 of 195\n","Starting to generate model responses for test conversation 50 of 195\n","Starting to generate model responses for test conversation 51 of 195\n","Starting to generate model responses for test conversation 52 of 195\n","Starting to generate model responses for test conversation 53 of 195\n","Starting to generate model responses for test conversation 54 of 195\n","Starting to generate model responses for test conversation 55 of 195\n","Starting to generate model responses for test conversation 56 of 195\n","Starting to generate model responses for test conversation 57 of 195\n","Starting to generate model responses for test conversation 58 of 195\n","Starting to generate model responses for test conversation 59 of 195\n","Starting to generate model responses for test conversation 60 of 195\n","Starting to generate model responses for test conversation 61 of 195\n","Starting to generate model responses for test conversation 62 of 195\n","Starting to generate model responses for test conversation 63 of 195\n","Starting to generate model responses for test conversation 64 of 195\n","Starting to generate model responses for test conversation 65 of 195\n","Starting to generate model responses for test conversation 66 of 195\n","Starting to generate model responses for test conversation 67 of 195\n","Starting to generate model responses for test conversation 68 of 195\n","Starting to generate model responses for test conversation 69 of 195\n","Starting to generate model responses for test conversation 70 of 195\n","Starting to generate model responses for test conversation 71 of 195\n","Starting to generate model responses for test conversation 72 of 195\n","Starting to generate model responses for test conversation 73 of 195\n","Starting to generate model responses for test conversation 74 of 195\n","Starting to generate model responses for test conversation 75 of 195\n","Starting to generate model responses for test conversation 76 of 195\n","Starting to generate model responses for test conversation 77 of 195\n","Starting to generate model responses for test conversation 78 of 195\n","Starting to generate model responses for test conversation 79 of 195\n","Starting to generate model responses for test conversation 80 of 195\n","Starting to generate model responses for test conversation 81 of 195\n","Starting to generate model responses for test conversation 82 of 195\n","Starting to generate model responses for test conversation 83 of 195\n","Starting to generate model responses for test conversation 84 of 195\n","Starting to generate model responses for test conversation 85 of 195\n","Starting to generate model responses for test conversation 86 of 195\n","Starting to generate model responses for test conversation 87 of 195\n","Starting to generate model responses for test conversation 88 of 195\n","Starting to generate model responses for test conversation 89 of 195\n","Starting to generate model responses for test conversation 90 of 195\n","Starting to generate model responses for test conversation 91 of 195\n","Starting to generate model responses for test conversation 92 of 195\n","Starting to generate model responses for test conversation 93 of 195\n","Starting to generate model responses for test conversation 94 of 195\n","Starting to generate model responses for test conversation 95 of 195\n","Starting to generate model responses for test conversation 96 of 195\n","Starting to generate model responses for test conversation 97 of 195\n","Starting to generate model responses for test conversation 98 of 195\n","Starting to generate model responses for test conversation 99 of 195\n","Starting to generate model responses for test conversation 100 of 195\n","Starting to generate model responses for test conversation 101 of 195\n","Starting to generate model responses for test conversation 102 of 195\n","Starting to generate model responses for test conversation 103 of 195\n","Starting to generate model responses for test conversation 104 of 195\n","Starting to generate model responses for test conversation 105 of 195\n","Starting to generate model responses for test conversation 106 of 195\n","Starting to generate model responses for test conversation 107 of 195\n","Starting to generate model responses for test conversation 108 of 195\n","Starting to generate model responses for test conversation 109 of 195\n","Starting to generate model responses for test conversation 110 of 195\n","Starting to generate model responses for test conversation 111 of 195\n","Starting to generate model responses for test conversation 112 of 195\n","Starting to generate model responses for test conversation 113 of 195\n","Starting to generate model responses for test conversation 114 of 195\n","Starting to generate model responses for test conversation 115 of 195\n","Starting to generate model responses for test conversation 116 of 195\n","Starting to generate model responses for test conversation 117 of 195\n","Starting to generate model responses for test conversation 118 of 195\n","Starting to generate model responses for test conversation 119 of 195\n","Starting to generate model responses for test conversation 120 of 195\n","Starting to generate model responses for test conversation 121 of 195\n","Starting to generate model responses for test conversation 122 of 195\n","Starting to generate model responses for test conversation 123 of 195\n","Starting to generate model responses for test conversation 124 of 195\n","Starting to generate model responses for test conversation 125 of 195\n","Starting to generate model responses for test conversation 126 of 195\n","Starting to generate model responses for test conversation 127 of 195\n","Starting to generate model responses for test conversation 128 of 195\n","Starting to generate model responses for test conversation 129 of 195\n","Starting to generate model responses for test conversation 130 of 195\n","Starting to generate model responses for test conversation 131 of 195\n","Starting to generate model responses for test conversation 132 of 195\n","Starting to generate model responses for test conversation 133 of 195\n","Starting to generate model responses for test conversation 134 of 195\n","Starting to generate model responses for test conversation 135 of 195\n","An error occurred: finish_reason: RECITATION\n","index: 0\n","\n","Starting to generate model responses for test conversation 136 of 195\n","Starting to generate model responses for test conversation 137 of 195\n","Starting to generate model responses for test conversation 138 of 195\n","Starting to generate model responses for test conversation 139 of 195\n","Starting to generate model responses for test conversation 140 of 195\n","Starting to generate model responses for test conversation 141 of 195\n","Starting to generate model responses for test conversation 142 of 195\n","Starting to generate model responses for test conversation 143 of 195\n","Starting to generate model responses for test conversation 144 of 195\n","Starting to generate model responses for test conversation 145 of 195\n","Starting to generate model responses for test conversation 146 of 195\n","Starting to generate model responses for test conversation 147 of 195\n","Starting to generate model responses for test conversation 148 of 195\n","Starting to generate model responses for test conversation 149 of 195\n","Starting to generate model responses for test conversation 150 of 195\n","Starting to generate model responses for test conversation 151 of 195\n","Starting to generate model responses for test conversation 152 of 195\n","Starting to generate model responses for test conversation 153 of 195\n","Starting to generate model responses for test conversation 154 of 195\n","Starting to generate model responses for test conversation 155 of 195\n","Starting to generate model responses for test conversation 156 of 195\n","Starting to generate model responses for test conversation 157 of 195\n","Starting to generate model responses for test conversation 158 of 195\n","Starting to generate model responses for test conversation 159 of 195\n","Starting to generate model responses for test conversation 160 of 195\n","Starting to generate model responses for test conversation 161 of 195\n","Starting to generate model responses for test conversation 162 of 195\n","Starting to generate model responses for test conversation 163 of 195\n","Starting to generate model responses for test conversation 164 of 195\n","Starting to generate model responses for test conversation 165 of 195\n","Starting to generate model responses for test conversation 166 of 195\n","Starting to generate model responses for test conversation 167 of 195\n","Starting to generate model responses for test conversation 168 of 195\n","Starting to generate model responses for test conversation 169 of 195\n","Starting to generate model responses for test conversation 170 of 195\n","Starting to generate model responses for test conversation 171 of 195\n","Starting to generate model responses for test conversation 172 of 195\n","Starting to generate model responses for test conversation 173 of 195\n","Starting to generate model responses for test conversation 174 of 195\n","Starting to generate model responses for test conversation 175 of 195\n","Starting to generate model responses for test conversation 176 of 195\n","Starting to generate model responses for test conversation 177 of 195\n","Starting to generate model responses for test conversation 178 of 195\n","Starting to generate model responses for test conversation 179 of 195\n","Starting to generate model responses for test conversation 180 of 195\n","Starting to generate model responses for test conversation 181 of 195\n","Starting to generate model responses for test conversation 182 of 195\n","Starting to generate model responses for test conversation 183 of 195\n","Starting to generate model responses for test conversation 184 of 195\n","Starting to generate model responses for test conversation 185 of 195\n","Starting to generate model responses for test conversation 186 of 195\n","Starting to generate model responses for test conversation 187 of 195\n","Starting to generate model responses for test conversation 188 of 195\n","Starting to generate model responses for test conversation 189 of 195\n","Starting to generate model responses for test conversation 190 of 195\n","Starting to generate model responses for test conversation 191 of 195\n","Starting to generate model responses for test conversation 192 of 195\n","Starting to generate model responses for test conversation 193 of 195\n","Starting to generate model responses for test conversation 194 of 195\n"]}],"source":["# Initialise list of lists for model responses and golden responses\n","all_model_responses = []\n","all_golden_responses = []\n","\n","# Iterate over all test conversations\n","for i in range(0, len(testdata)):\n","\n","  # Print statement\n","  print(f'Starting to generate model responses for test conversation {i} of {len(testdata)}')\n","\n","  # Get dialog data\n","  dialog = testdata[i]['dialog']\n","\n","  # Get seeker_list, supporter_list and who starts the conversation\n","  seeker_list, supporter_list, first_speaker = sort_and_track_dialog(dialog)\n","\n","  # Create chat history\n","  chat_history = create_chat_history(prompt, model_start, seeker_list, supporter_list, first_speaker)\n","\n","  # Generate model responses\n","  model_responses, golden_responses = generate_model_responses(dialog, prompt, model_start)\n","\n","  # Add model and golden responses to list of lists\n","  all_model_responses.append(model_responses)\n","  all_golden_responses.append(golden_responses)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":276,"status":"ok","timestamp":1712164607207,"user":{"displayName":"Niclas Grießhaber","userId":"09526291260391729561"},"user_tz":-60},"id":"fsBiHPdlPRzs","outputId":"c21ab9e0-5be0-4583-8a4d-fb3bb8cc0851"},"outputs":[{"output_type":"stream","name":"stdout","text":["All model and golden responses were saved.\n"]}],"source":["# Save all_model_responses and all_golden_responses to disk\n","with open(save_model_responses, 'wb') as f:\n","    pickle.dump(all_model_responses, f)\n","\n","with open(save_golden_responses, 'wb') as f:\n","    pickle.dump(all_golden_responses, f)\n","\n","print('All model and golden responses were saved.')"]},{"cell_type":"markdown","metadata":{"id":"RjtPcMHkpq5q"},"source":["# Load model responses"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2,"status":"ok","timestamp":1712178245838,"user":{"displayName":"Niclas Grießhaber","userId":"09526291260391729561"},"user_tz":-60},"id":"YZjmfsDQpsav","outputId":"9000edfe-e0ae-4feb-e2b9-c7eddb6c0b4e"},"outputs":[{"output_type":"stream","name":"stdout","text":["All model and golden responses were loaded.\n"]}],"source":["# Load all_model_responses from disk\n","with open(save_model_responses, 'rb') as f:\n","    all_model_responses = pickle.load(f)\n","\n","# Load all_golden_responses from disk\n","with open(save_golden_responses, 'rb') as f:\n","    all_golden_responses = pickle.load(f)\n","\n","print('All model and golden responses were loaded.')"]},{"cell_type":"markdown","metadata":{"id":"EVE07xdaDyRd"},"source":["# Flatten responses for evaluation"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bYpwxNpmD0hv"},"outputs":[],"source":["model_response_flattened = [item for sublist in all_model_responses for item in sublist]\n","conversation_golden_responses_flattened = [item for sublist in all_golden_responses for item in sublist]"]},{"cell_type":"markdown","metadata":{"id":"abBCdQMzyyEG"},"source":["# Evaluation with BLEU"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RcDLWwhxyxeG"},"outputs":[],"source":["def calculate_bleu_scores(model_responses, golden_responses, is_corpus=False):\n","\n","    # Ensure that the lengths of model responses and golden responses are the same\n","    assert len(model_responses) == len(golden_responses), \"The lengths of model responses and golden responses should match.\"\n","\n","    # Initialize lists to store individual sentence scores for each BLEU n-gram\n","    bleu_1_scores = []\n","    bleu_2_scores = []\n","    bleu_3_scores = []\n","    bleu_4_scores = []\n","\n","    # Calculate sentence BLEU scores for each response\n","    for model_response, golden_response in zip(model_responses, golden_responses):\n","        reference = [golden_response.split()]  # Tokenize the golden response\n","        candidate = model_response.split()     # Tokenize the model response\n","\n","        # Calculate and store BLEU scores for each sentence\n","        bleu_1_scores.append(sentence_bleu(reference, candidate, weights=(1, 0, 0, 0), smoothing_function=SmoothingFunction().method1))\n","        bleu_2_scores.append(sentence_bleu(reference, candidate, weights=(0.5, 0.5, 0, 0), smoothing_function=SmoothingFunction().method1))\n","        bleu_3_scores.append(sentence_bleu(reference, candidate, weights=(0.33, 0.33, 0.33, 0), smoothing_function=SmoothingFunction().method1))\n","        bleu_4_scores.append(sentence_bleu(reference, candidate, weights=(0.25, 0.25, 0.25, 0.25), smoothing_function=SmoothingFunction().method1))\n","\n","    # Calculate average BLEU scores across all responses\n","    avg_bleu_1 = sum(bleu_1_scores) / len(bleu_1_scores)\n","    avg_bleu_2 = sum(bleu_2_scores) / len(bleu_2_scores)\n","    avg_bleu_3 = sum(bleu_3_scores) / len(bleu_3_scores)\n","    avg_bleu_4 = sum(bleu_4_scores) / len(bleu_4_scores)\n","\n","    return avg_bleu_1, avg_bleu_2, avg_bleu_3, avg_bleu_4"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2577,"status":"ok","timestamp":1712178327281,"user":{"displayName":"Niclas Grießhaber","userId":"09526291260391729561"},"user_tz":-60},"id":"rG8SuqNf4KQT","outputId":"a032ae54-39c6-481a-f523-424eca5ef2dc"},"outputs":[{"output_type":"stream","name":"stdout","text":["Average BLEU-1 score: 9.948840934544801\n","Average BLEU-2 score: 2.9352130299880823\n","Average BLEU-3 score: 1.50297647761927\n","Average BLEU-4 score: 0.9452084170030023\n"]}],"source":["avg_bleu_1, avg_bleu_2, avg_bleu_3, avg_bleu_4 = calculate_bleu_scores(model_response_flattened, conversation_golden_responses_flattened)\n","\n","print(\"Average BLEU-1 score:\", avg_bleu_1 * 100)\n","print(\"Average BLEU-2 score:\", avg_bleu_2 * 100)\n","print(\"Average BLEU-3 score:\", avg_bleu_3 * 100)\n","print(\"Average BLEU-4 score:\", avg_bleu_4 * 100)"]},{"cell_type":"markdown","metadata":{"id":"LTIuV0KY4k9q"},"source":["# Meteor"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZMStM2VL4KXo"},"outputs":[],"source":["def calculate_meteor(candidate, reference):\n","  '''\n","  candidate, reference: tokenized list of words in the sentence\n","  '''\n","  mt_list = []\n","  mt_sum = 0\n","  for c, r in zip(candidate, reference):\n","    r_tokenized = word_tokenize(r)\n","    c_tokenized = word_tokenize(c)\n","    meteor_score = round(meteor([c_tokenized], r_tokenized), 4)\n","    #print(\"Model output: \", c)\n","    #print(\"Gold reference: \", r)\n","    #print(\"Meteor score: \", meteor_score)\n","    mt_list.append(meteor_score)\n","    mt_sum += meteor_score\n","    #print(\"\\n\")\n","  avg_mt = mt_sum / len(mt_list)\n","  print(f\"Total average meteor score: {str(avg_mt)}\")\n","  return meteor_score"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":12102,"status":"ok","timestamp":1712178339382,"user":{"displayName":"Niclas Grießhaber","userId":"09526291260391729561"},"user_tz":-60},"id":"nYXB0NCjDllg","outputId":"a1602dcb-002a-46af-eba9-5366186c2d71"},"outputs":[{"output_type":"stream","name":"stdout","text":["Total average meteor score: 0.10695113373438238\n"]},{"output_type":"execute_result","data":{"text/plain":["0.1535"]},"metadata":{},"execution_count":15}],"source":["calculate_meteor(model_response_flattened, conversation_golden_responses_flattened)"]},{"cell_type":"markdown","metadata":{"id":"BhSQn7rj5E8T"},"source":["# Rouge"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qxLOq0Z24m6W"},"outputs":[],"source":["# Initialize the scorer\n","scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n","\n","# Initialize sums for each ROUGE score\n","sum_rougeL_precision, sum_rougeL_recall, sum_rougeL_fmeasure = 0, 0, 0\n","\n","# Calculate scores for each sentence pair\n","for m, c in zip(model_response_flattened, conversation_golden_responses_flattened):\n","    score = scorer.score(m, c)\n","\n","    # Accumulate the scores\n","    sum_rougeL_precision += score[\"rougeL\"].precision\n","    sum_rougeL_recall += score[\"rougeL\"].recall\n","    sum_rougeL_fmeasure += score[\"rougeL\"].fmeasure\n","\n","# Calculate the averages\n","avg_rougeL_precision = sum_rougeL_precision / len(model_response_flattened)\n","avg_rougeL_recall = sum_rougeL_recall / len(model_response_flattened)\n","avg_rougeL_fmeasure = sum_rougeL_fmeasure / len(model_response_flattened)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3,"status":"ok","timestamp":1712178349633,"user":{"displayName":"Niclas Grießhaber","userId":"09526291260391729561"},"user_tz":-60},"id":"pk5t7j254m8t","outputId":"dee5729e-8f82-4192-9e46-9792c11e1eb6"},"outputs":[{"output_type":"stream","name":"stdout","text":["Average ROUGE-L Precision: 0.24231084010987666\n","Average ROUGE-L Recall: 0.09718937290911356\n","Average ROUGE-L F-measure: 0.12655826933857547\n"]}],"source":["# Print the average scores\n","print(f'Average ROUGE-L Precision: {avg_rougeL_precision}')\n","print(f'Average ROUGE-L Recall: {avg_rougeL_recall}')\n","print(f'Average ROUGE-L F-measure: {avg_rougeL_fmeasure}')"]},{"cell_type":"markdown","source":["# Distinct"],"metadata":{"id":"Utft4o14_ZFd"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"SanZsrMaD7gV","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1712178350667,"user_tz":-60,"elapsed":1036,"user":{"displayName":"Niclas Grießhaber","userId":"09526291260391729561"}},"outputId":"75defada-f8f9-4587-cfc4-8f8915700110"},"outputs":[{"output_type":"stream","name":"stdout","text":["Cloning into 'Distinct-N'...\n","remote: Enumerating objects: 79, done.\u001b[K\n","remote: Counting objects: 100% (15/15), done.\u001b[K\n","remote: Compressing objects: 100% (9/9), done.\u001b[K\n","remote: Total 79 (delta 6), reused 12 (delta 6), pack-reused 64\u001b[K\n","Receiving objects: 100% (79/79), 186.03 KiB | 2.21 MiB/s, done.\n","Resolving deltas: 100% (28/28), done.\n","/content/Distinct-N\n"]}],"source":["# Clone the Distinct-N repository\n","!git clone https://github.com/neural-dialogue-metrics/Distinct-N.git\n","%cd Distinct-N\n","\n","from distinct_n.utils import ngrams\n","\n","def distinct_n_sentence_level(sentence, n):\n","    \"\"\"\n","    Compute distinct-N for a single sentence.\n","    :param sentence: a list of words.\n","    :param n: int, ngram.\n","    :return: float, the metric value.\n","    \"\"\"\n","    if len(sentence) == 0:\n","        return 0.0  # Prevent a zero division\n","    distinct_ngrams = set(ngrams(sentence, n))\n","    return len(distinct_ngrams) / len(sentence)\n","\n","def distinct_n_corpus_level(sentences, n):\n","    \"\"\"\n","    Compute average distinct-N of a list of sentences (the corpus).\n","    :param sentences: a list of sentence.\n","    :param n: int, ngram.\n","    :return: float, the average value.\n","    \"\"\"\n","    return sum(distinct_n_sentence_level(sentence, n) for sentence in sentences) / len(sentences)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4WXj7gMND7iZ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1712178351274,"user_tz":-60,"elapsed":608,"user":{"displayName":"Niclas Grießhaber","userId":"09526291260391729561"}},"outputId":"1d10b7cd-b177-43f2-de74-997c2a3e2513"},"outputs":[{"output_type":"stream","name":"stdout","text":["Distinct 1: 0.12314263432794803\n"]}],"source":["distinct_1_list = []\n","\n","for response in model_response_flattened:\n","    d_1 = distinct_n_sentence_level(response, 1)  # or adjust max_length as needed\n","    distinct_1_list.append(d_1)\n","\n","# Now, 'perplexities' contains the perplexity for each response.\n","# You can print them out or analyze them further as needed.\n","#print(distinct_1_list)\n","\n","d_1 = sum(distinct_1_list) / len(distinct_1_list)\n","print(f'Distinct 1: {d_1}')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EUfNng1UD7kf","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1712178351779,"user_tz":-60,"elapsed":506,"user":{"displayName":"Niclas Grießhaber","userId":"09526291260391729561"}},"outputId":"2b7f094c-382e-402f-e983-3fb00d258827"},"outputs":[{"output_type":"stream","name":"stdout","text":["Distinct 2: 0.537523993167126\n"]}],"source":["distinct_2_list = []\n","\n","for response in model_response_flattened:\n","    d_2 = distinct_n_sentence_level(response, 2)  # or adjust max_length as needed\n","    distinct_2_list.append(d_2)\n","\n","# Now, 'perplexities' contains the perplexity for each response.\n","# You can print them out or analyze them further as needed.\n","#print(distinct_2_list)\n","\n","d_2 = sum(distinct_2_list) / len(distinct_2_list)\n","print(f'Distinct 2: {d_2}')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"55A4nW8iD7mU"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RbY-_u277zZ7"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3-sqJGxX7zcB"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sME5gqvr7zeG"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3,"status":"ok","timestamp":1711309180640,"user":{"displayName":"Niclas Grießhaber","userId":"09526291260391729561"},"user_tz":0},"id":"Cgou5WAC7_pS","outputId":"14fa063d-5099-4f9b-9618-b0f43d3e87ba"},"outputs":[{"data":{"text/plain":["[{'speaker': 'seeker', 'content': 'Hello good afternoon.'},\n"," {'speaker': 'supporter',\n","  'strategy': 'Question',\n","  'content': 'Hi, good afternoon.'},\n"," {'speaker': 'seeker',\n","  'content': \"I'm feeling anxious that I am going to lose my job.\"},\n"," {'speaker': 'supporter',\n","  'strategy': 'Reflection of feelings',\n","  'content': 'Losing a job is always anxious.'},\n"," {'speaker': 'seeker', 'content': \"I hope I don't.\"},\n"," {'speaker': 'supporter',\n","  'strategy': 'Question',\n","  'content': 'Why do you think you will lose your job?'},\n"," {'speaker': 'seeker',\n","  'content': 'I am on short term disability and I am not ready to go back to work yet but I do not have any job protection.'},\n"," {'speaker': 'supporter',\n","  'strategy': 'Restatement or Paraphrasing',\n","  'content': 'Oh so your job is not protected and your short term disability will end soon? Is that correct?'},\n"," {'speaker': 'seeker',\n","  'content': \"It's not ending yet, but no my job is not protected. I live in the United States, but I have not been at my job long enough to earn protection for medical leave.\"},\n"," {'speaker': 'seeker',\n","  'content': 'you have to have been here for a year, and I started November 2020'},\n"," {'speaker': 'seeker',\n","  'content': \"I'm afraid that I will lose my job since I'm still on disability for the foreseeable future.\"},\n"," {'speaker': 'supporter',\n","  'strategy': 'Providing Suggestions',\n","  'content': 'I see. Have you spoken to HR?'},\n"," {'speaker': 'seeker',\n","  'content': \"I have, but they are telling me that it is up to my department manager who isn't actually getting back to me about it yet.\"},\n"," {'speaker': 'supporter',\n","  'strategy': 'Restatement or Paraphrasing',\n","  'content': 'Your department manager is not answering you?'},\n"," {'speaker': 'seeker',\n","  'content': 'No, I have sent them a few emails about it. It makes me nervous. I do not have a phone number to call and my psychiatrist really does not think I am ready to go back to the stress of my job.'},\n"," {'speaker': 'seeker',\n","  'content': 'I wish I could just call him, but I do not have a phone number for him. Just his email.'},\n"," {'speaker': 'supporter',\n","  'strategy': 'Providing Suggestions',\n","  'content': 'Have you tried mentioning that to HR?'},\n"," {'speaker': 'seeker', 'content': 'HR is the one who gave me his email.'},\n"," {'speaker': 'seeker',\n","  'content': \"I don't think he has a direct work line, and they're not allowed to give out personal information.\"},\n"," {'speaker': 'supporter',\n","  'strategy': 'Information',\n","  'content': 'Yes that is how most employments work about providing personal information.'},\n"," {'speaker': 'supporter',\n","  'strategy': 'Reflection of feelings',\n","  'content': 'That in itself causes anxiety to most since other forms of communication is not possible especially if they are not responding to ciritcal matters.'},\n"," {'speaker': 'seeker',\n","  'content': \"Yes! 100%. Every time my phone rings I get nervous that I'm being fired and I'm worried I made a mistake going out on disability. I needed to though, but I'm nervous.\"},\n"," {'speaker': 'supporter',\n","  'strategy': 'Affirmation and Reassurance',\n","  'content': 'No you should not have to feel you made a mistake for the time you are taking out of work for a necessity.'},\n"," {'speaker': 'seeker',\n","  'content': \"Thank you for saying that. That does make me feel better. It sucks sometimes we have to choose between our health and our jobs. We don't get paid to take care of our health though, we only get paid to work.\"},\n"," {'speaker': 'supporter',\n","  'strategy': 'Others',\n","  'content': 'You know yourself more than anybody and you need to take care of yourself.'}]"]},"execution_count":30,"metadata":{},"output_type":"execute_result"}],"source":["traindata[0]['dialog']"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"elapsed":192,"status":"ok","timestamp":1711309196565,"user":{"displayName":"Niclas Grießhaber","userId":"09526291260391729561"},"user_tz":0},"id":"nvLjZBPs8b_h","outputId":"241cd849-84c2-4c2d-a9b9-31fb98e110a5"},"outputs":[{"data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'extracted_dialogs.txt'"]},"execution_count":31,"metadata":{},"output_type":"execute_result"}],"source":["# Adjusted function to merge consecutive messages from the same speaker\n","def format_dialog(dialog):\n","    formatted_dialog = []\n","    current_speaker = \"\"\n","    current_content = []\n","    for message in dialog:\n","        speaker_role = \"User\" if message[\"speaker\"] == \"seeker\" else \"Model\"\n","        if speaker_role == current_speaker:\n","            # If the current speaker is the same as the last, append the content\n","            current_content.append(message['content'].strip())\n","        else:\n","            # If the speaker changes, join the current content and start a new block\n","            if current_content:\n","                formatted_dialog.append(f\"{current_speaker}: {' '.join(current_content)}\")\n","            current_speaker = speaker_role\n","            current_content = [message['content'].strip()]\n","    # Don't forget to add the last block of messages\n","    if current_content:\n","        formatted_dialog.append(f\"{current_speaker}: {' '.join(current_content)}\")\n","    return \"\\n\".join(formatted_dialog)\n","\n","# Extract and format the specified dialogs\n","extracted_dialogs = []\n","for index in range(len(traindata)):\n","    formatted_dialog = format_dialog(traindata[index]['dialog'])\n","    extracted_dialogs.append(f\"Example conversation #{index + 1}\\n{formatted_dialog}\")\n","\n","# Save the extracted dialogs to a .txt file\n","file_path = \"extracted_dialogs.txt\"  # Specify your desired file path\n","with open(file_path, \"w\") as file:\n","    file.write(\"\\n\\n\".join(extracted_dialogs))\n","\n","file_path  # Return the path of the saved file for reference"]}],"metadata":{"colab":{"provenance":[],"mount_file_id":"1JHeGjhFidzRotc3IKUYZuvtB5hhlsGyk","authorship_tag":"ABX9TyPpOB30IfwLxNCpYVO3OVTp"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}